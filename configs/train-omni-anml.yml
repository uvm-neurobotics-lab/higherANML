#
# Config for standard training of ANML for continuous learning on Omniglot.
#

# Dataset
dataset: omni
train_size: 20

# Training Configuration
train_method: meta
sample_method: single
batch_size: 1
num_batches: 20
train_cycles: 1
val_sample_size: 0
remember_size: 64
remember_only: false
inner_lr: 0.1
outer_lr: 0.001

epochs: 25000
seed: 1

# Model Architecture
model: classifier
model_name: anml
model_args:
  encoder: anml-encoder
  encoder_args:
    rln_chs: 256
    nm_chs: 112
    pool_rln_output: false
  classifier: linear-classifier
  classifier_args: {num_classes: 1000}

# Optimization Configuration
inner_params: [encoder.rln, classifier]
outer_params: all
output_layer: classifier.linear

eval_steps: [10000, 25000]
eval:
  - no-sgd: !include eval-omni-zero-shot.yml
  - olft: !include eval-sweep-omni-1FC.yml
  - unfrozen: !include eval-sweep-omni-unfrozen.yml
  - iid-olft: !include eval-sweep-omni-iid-1FC.yml
  - iid-unfrozen: !include eval-sweep-omni-iid-unfrozen.yml
